{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is =  0.5 \n",
      "\n",
      "--->epoch=0, error=6.35013\n",
      "--->epoch=1, error=5.53148\n",
      "--->epoch=2, error=5.22117\n",
      "--->epoch=3, error=4.95138\n",
      "--->epoch=4, error=4.51894\n",
      "--->epoch=5, error=4.17304\n",
      "--->epoch=6, error=3.83494\n",
      "--->epoch=7, error=3.50573\n",
      "--->epoch=8, error=3.19212\n",
      "--->epoch=9, error=2.89767\n",
      "--->epoch=10, error=2.62554\n",
      "--->epoch=11, error=2.37744\n",
      "--->epoch=12, error=2.15348\n",
      "--->epoch=13, error=1.95270\n",
      "--->epoch=14, error=1.77352\n",
      "--->epoch=15, error=1.61401\n",
      "--->epoch=16, error=1.47211\n",
      "--->epoch=17, error=1.34580\n",
      "--->epoch=18, error=1.23315\n",
      "--->epoch=19, error=1.13241\n",
      "--->epoch=20, error=1.04204\n",
      "--->epoch=21, error=0.96072\n",
      "--->epoch=22, error=0.88737\n",
      "--->epoch=23, error=0.82108\n",
      "--->epoch=24, error=0.76109\n",
      "--->epoch=25, error=0.70677\n",
      "--->epoch=26, error=0.65757\n",
      "--->epoch=27, error=0.61299\n",
      "--->epoch=28, error=0.57257\n",
      "--->epoch=29, error=0.53590\n",
      "--->epoch=30, error=0.50261\n",
      "--->epoch=31, error=0.47233\n",
      "--->epoch=32, error=0.44475\n",
      "--->epoch=33, error=0.41960\n",
      "--->epoch=34, error=0.39662\n",
      "--->epoch=35, error=0.37558\n",
      "--->epoch=36, error=0.35628\n",
      "--->epoch=37, error=0.33855\n",
      "--->epoch=38, error=0.32222\n",
      "--->epoch=39, error=0.30716\n",
      "--->epoch=40, error=0.29323\n",
      "--->epoch=41, error=0.28034\n",
      "--->epoch=42, error=0.26837\n",
      "--->epoch=43, error=0.25725\n",
      "--->epoch=44, error=0.24689\n",
      "--->epoch=45, error=0.23724\n",
      "--->epoch=46, error=0.22821\n",
      "--->epoch=47, error=0.21977\n",
      "--->epoch=48, error=0.21186\n",
      "--->epoch=49, error=0.20443\n",
      "--->epoch=50, error=0.19745\n",
      "--->epoch=51, error=0.19088\n",
      "--->epoch=52, error=0.18468\n",
      "--->epoch=53, error=0.17884\n",
      "--->epoch=54, error=0.17332\n",
      "--->epoch=55, error=0.16809\n",
      "--->epoch=56, error=0.16314\n",
      "--->epoch=57, error=0.15844\n",
      "--->epoch=58, error=0.15399\n",
      "--->epoch=59, error=0.14975\n",
      "--->epoch=60, error=0.14572\n",
      "--->epoch=61, error=0.14189\n",
      "--->epoch=62, error=0.13823\n",
      "--->epoch=63, error=0.13474\n",
      "--->epoch=64, error=0.13141\n",
      "--->epoch=65, error=0.12822\n",
      "--->epoch=66, error=0.12518\n",
      "--->epoch=67, error=0.12226\n",
      "--->epoch=68, error=0.11947\n",
      "--->epoch=69, error=0.11679\n",
      "--->epoch=70, error=0.11422\n",
      "--->epoch=71, error=0.11175\n",
      "--->epoch=72, error=0.10938\n",
      "--->epoch=73, error=0.10710\n",
      "--->epoch=74, error=0.10491\n",
      "--->epoch=75, error=0.10280\n",
      "--->epoch=76, error=0.10076\n",
      "--->epoch=77, error=0.09880\n",
      "--->epoch=78, error=0.09691\n",
      "--->epoch=79, error=0.09509\n",
      "--->epoch=80, error=0.09333\n",
      "--->epoch=81, error=0.09163\n",
      "--->epoch=82, error=0.08998\n",
      "--->epoch=83, error=0.08839\n",
      "--->epoch=84, error=0.08685\n",
      "--->epoch=85, error=0.08536\n",
      "--->epoch=86, error=0.08392\n",
      "--->epoch=87, error=0.08253\n",
      "--->epoch=88, error=0.08117\n",
      "--->epoch=89, error=0.07986\n",
      "--->epoch=90, error=0.07859\n",
      "--->epoch=91, error=0.07735\n",
      "--->epoch=92, error=0.07615\n",
      "--->epoch=93, error=0.07498\n",
      "--->epoch=94, error=0.07385\n",
      "--->epoch=95, error=0.07275\n",
      "--->epoch=96, error=0.07168\n",
      "--->epoch=97, error=0.07064\n",
      "--->epoch=98, error=0.06963\n",
      "--->epoch=99, error=0.06864\n",
      "--->epoch=100, error=0.06769\n",
      "--->epoch=101, error=0.06675\n",
      "--->epoch=102, error=0.06584\n",
      "--->epoch=103, error=0.06495\n",
      "--->epoch=104, error=0.06409\n",
      "--->epoch=105, error=0.06325\n",
      "--->epoch=106, error=0.06243\n",
      "--->epoch=107, error=0.06162\n",
      "--->epoch=108, error=0.06084\n",
      "--->epoch=109, error=0.06008\n",
      "--->epoch=110, error=0.05933\n",
      "--->epoch=111, error=0.05860\n",
      "--->epoch=112, error=0.05789\n",
      "--->epoch=113, error=0.05719\n",
      "--->epoch=114, error=0.05651\n",
      "--->epoch=115, error=0.05585\n",
      "--->epoch=116, error=0.05520\n",
      "--->epoch=117, error=0.05456\n",
      "--->epoch=118, error=0.05394\n",
      "--->epoch=119, error=0.05333\n",
      "--->epoch=120, error=0.05274\n",
      "--->epoch=121, error=0.05216\n",
      "--->epoch=122, error=0.05159\n",
      "--->epoch=123, error=0.05103\n",
      "--->epoch=124, error=0.05048\n",
      "--->epoch=125, error=0.04994\n",
      "--->epoch=126, error=0.04942\n",
      "--->epoch=127, error=0.04890\n",
      "--->epoch=128, error=0.04840\n",
      "--->epoch=129, error=0.04790\n",
      "--->epoch=130, error=0.04742\n",
      "--->epoch=131, error=0.04694\n",
      "--->epoch=132, error=0.04647\n",
      "--->epoch=133, error=0.04602\n",
      "--->epoch=134, error=0.04557\n",
      "--->epoch=135, error=0.04513\n",
      "--->epoch=136, error=0.04469\n",
      "--->epoch=137, error=0.04427\n",
      "--->epoch=138, error=0.04385\n",
      "--->epoch=139, error=0.04344\n",
      "--->epoch=140, error=0.04304\n",
      "--->epoch=141, error=0.04264\n",
      "--->epoch=142, error=0.04225\n",
      "--->epoch=143, error=0.04187\n",
      "--->epoch=144, error=0.04150\n",
      "--->epoch=145, error=0.04113\n",
      "--->epoch=146, error=0.04076\n",
      "--->epoch=147, error=0.04041\n",
      "--->epoch=148, error=0.04006\n",
      "--->epoch=149, error=0.03971\n",
      "--->epoch=150, error=0.03937\n",
      "--->epoch=151, error=0.03904\n",
      "--->epoch=152, error=0.03871\n",
      "--->epoch=153, error=0.03839\n",
      "--->epoch=154, error=0.03807\n",
      "--->epoch=155, error=0.03776\n",
      "--->epoch=156, error=0.03745\n",
      "--->epoch=157, error=0.03715\n",
      "--->epoch=158, error=0.03685\n",
      "--->epoch=159, error=0.03655\n",
      "--->epoch=160, error=0.03626\n",
      "--->epoch=161, error=0.03598\n",
      "--->epoch=162, error=0.03570\n",
      "--->epoch=163, error=0.03542\n",
      "--->epoch=164, error=0.03515\n",
      "--->epoch=165, error=0.03488\n",
      "--->epoch=166, error=0.03462\n",
      "--->epoch=167, error=0.03436\n",
      "--->epoch=168, error=0.03410\n",
      "--->epoch=169, error=0.03385\n",
      "--->epoch=170, error=0.03360\n",
      "--->epoch=171, error=0.03335\n",
      "--->epoch=172, error=0.03311\n",
      "--->epoch=173, error=0.03287\n",
      "--->epoch=174, error=0.03263\n",
      "--->epoch=175, error=0.03240\n",
      "--->epoch=176, error=0.03217\n",
      "--->epoch=177, error=0.03195\n",
      "--->epoch=178, error=0.03172\n",
      "--->epoch=179, error=0.03150\n",
      "--->epoch=180, error=0.03128\n",
      "--->epoch=181, error=0.03107\n",
      "--->epoch=182, error=0.03086\n",
      "--->epoch=183, error=0.03065\n",
      "--->epoch=184, error=0.03044\n",
      "--->epoch=185, error=0.03024\n",
      "--->epoch=186, error=0.03004\n",
      "--->epoch=187, error=0.02984\n",
      "--->epoch=188, error=0.02965\n",
      "--->epoch=189, error=0.02945\n",
      "--->epoch=190, error=0.02926\n",
      "--->epoch=191, error=0.02907\n",
      "--->epoch=192, error=0.02889\n",
      "--->epoch=193, error=0.02870\n",
      "--->epoch=194, error=0.02852\n",
      "--->epoch=195, error=0.02834\n",
      "--->epoch=196, error=0.02816\n",
      "--->epoch=197, error=0.02799\n",
      "--->epoch=198, error=0.02782\n",
      "--->epoch=199, error=0.02765\n",
      "[{'weights': [-1.9840720697674477, 2.753255094433545, 1.427688378413599], 'output': 0.015845027117728623, 'delta': -0.0002410319781990047}, {'weights': [1.1390666350663963, -1.692904205238224, -0.3243968738600616], 'output': 0.9210833874002823, 'delta': 0.0005458208693962861}] \n",
      "\n",
      "[{'weights': [4.928084483306895, -2.0628451404591988, -1.314165888192279], 'output': 0.041702700400936064, 'delta': -0.0016665894197174123}, {'weights': [-4.5840586831162655, 2.5778944975940523, 0.8626042569273801], 'output': 0.959416547504382, 'delta': 0.0015801749958503201}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from random import seed,random\n",
    "\n",
    "def initialize_Neural_net(no_inputs, no_layers, no_outputs):\n",
    "    Neural_net = list()\n",
    "    internal_layer = [{'weights':[random() for i in range(no_inputs + 1)]} for i in range(no_layers)]\n",
    "    Neural_net.append(internal_layer)\n",
    "    outputs_layer = [{'weights':[random() for i in range(no_layers + 1)]} for i in range(no_outputs)]\n",
    "    Neural_net.append(outputs_layer)\n",
    "    return Neural_net\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + exp(-x))\n",
    "\n",
    "def Activation(no_weights, no_inputs):\n",
    "    activation = no_weights[-1]\n",
    "    for i in range(len(no_weights)-1):\n",
    "        activation += no_weights[i] * no_inputs[i]\n",
    "    return activation\n",
    "\n",
    "def forward_pass(neural_net, instance):\n",
    "    no_inputs = instance\n",
    "    for i in neural_net:\n",
    "        new_no_inputs = []\n",
    "        for j in i:\n",
    "            activation = Activation(j['weights'], no_inputs)\n",
    "            j['output'] = sigmoid(activation)\n",
    "            new_no_inputs.append(j['output'])\n",
    "        no_inputs = new_no_inputs\n",
    "    return no_inputs\n",
    "\n",
    "def sigmoid_derivative(result):\n",
    "    return result * (1.0 - result)\n",
    "\n",
    "def backward_pass(neural_net, expected_res):\n",
    "    for i in reversed(range(len(neural_net))):\n",
    "        layer = neural_net[i]\n",
    "        errors = list()\n",
    "        if i != len(neural_net)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in neural_net[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected_res[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * sigmoid_derivative(neuron['output'])\n",
    "\n",
    "def weights_update(neural_net, instance, learning_rate):\n",
    "    for i in range(len(neural_net)):\n",
    "        inputs = instance[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in neural_net[i - 1]]\n",
    "        for neuron in neural_net[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += learning_rate * neuron['delta']\n",
    "\n",
    "def train(neural_net, train, learning_rate, no_epoch, no_outputs):\n",
    "    print('Learning Rate is = ' ,(learning_rate), '\\n')\n",
    "    for i in range(no_epoch):\n",
    "        error_sum = 0\n",
    "        for j in train:\n",
    "            outputs = forward_pass(neural_net, j)\n",
    "            result = [0 for i in range(no_outputs)]\n",
    "            result[j[-1]] = 1\n",
    "            error_sum += sum([(result[i]-outputs[i])**2 for i in range(len(result))])\n",
    "            backward_pass(neural_net, result)\n",
    "            weights_update(neural_net, j, learning_rate)\n",
    "        print('--->epoch=%d, error=%.5f' % (i,error_sum))\n",
    "\n",
    "seed(1)\n",
    "data = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "no_inputs = len(data[0]) - 1\n",
    "no_outputs = len(set([instance[-1] for instance in data]))\n",
    "Neural_net = initialize_Neural_net(no_inputs, 2, no_outputs)\n",
    "train(Neural_net, data, 0.5, 200, no_outputs)\n",
    "for layer in Neural_net:\n",
    "    print(layer,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
